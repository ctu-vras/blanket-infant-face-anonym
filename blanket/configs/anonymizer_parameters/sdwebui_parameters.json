anonymization_padding_method: PaddingMethod = "ratio"
anonymization_padding_ratio: float = 0.75
anonymization_padding_constant: int = 96

{
     eval !!! compare no prompt/no celebrity prompt/(full prompt)
    "prompt": "(photo of a little baby face: 1.2)",  # (Daryl Sabara: 0.2), (Macaulay Culkin: 0.1), (Thomas Sangster: 0.1), (Kelly Macdonald: 0.1), (Taylor Swift: 0.2), (Sydney Sweeney: 0.2),
    # eval compare no negative prompt/(full negative prompt)
    "negative_prompt": "(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, painting, black and white, bubble gum, face mask), text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck",
    "steps": 30,  # eval? (not as important)
    "width": 896,  # 512, (896), 1024    eval
    "height": 896,  # 512, (896), 1024    eval
    "resize_mode": 0,
    "sampler_name": "DPM++ SDE Karras",  # eval? (not as important)
    "cfg_scale": 4,  # eval 2, (4), 8, 12
    "initial_noise_multiplier": 1,
    "denoising_strength": 0.7,   # eval !!!!!  0.4, 0.5, 0.6, (0.7), 0.8, 0.9
    "n_iter": 1,
    "init_images": [image_byte64],
    "batch_size": 1,
    "mask": mask_byte64,  # eval? (comparing different masks is hard)
    # mask blur shouldn't be too high because then the new image doesn't fit with the old one and it is visible when blended together
    "mask_blur": 4,  # eval 1, (4), 8, 16
    # "mask_blur_x": 4,
    # "mask_blur_y": 4,
    "mask_mode": 1,  # 'Inpaint masked', 'Inpaint not masked'
    "inpainting_fill": 1,  # 'fill', 'original', 'latent noise', 'latent nothing'    eval? (hard to evaluate because each one needs different other settings)
    "inpaint_full_res": 1,  # "Whole picture", "Only masked"    eval? (not as important)
    "inpaint_full_res_padding": padding,    # eval !  64, (96), 144, coeffs?
    "inpainting_mask_invert": 0,
    "override_settings": {  # eval? (evaluated partially by varying time of refiner switch)
        'sd_model_checkpoint': "RealisticVisionV20.safetensors",  # this can be used to switch sd model
    },
    "seed": 1,
    "subseed": -1,
    "subseed_strength": 0,
    "seed_enable_extras": True,
    "seed_resize_from_h": -1,
    "seed_resize_from_w": -1,
    "tiling": False,
    "styles": [],
    "restore_faces": False,  # eval restore_faces on, (restore_faces off)
    "script_args": [],
    "script_name": None,
    "refiner_switch_at": 0.4,  # eval  0.0, 0.2, (0.4), 0.6, 1.0
    "alwayson_scripts": {
        "API payload": {"args": []},
        "CodeFormer": {"args": [0, 0]},
        "ControlNet": {"args": [  # eval compare no ControlNet, (with ControlNet)
            {  # ControlNet OpenPose
                "advanced_weighting": None,
                "batch_images": "",
                "control_mode": "ControlNet is more important",
                "enabled": True,
                "guidance_end": 1,  # at which part of the generation does controlnet stop affecting the output
                "guidance_start": 0,  # at which part of the generation does controlnet start affecting the output
                "hr_option": "Both",
                "image": None,
                "inpaint_crop_input_image": True,
                "input_mode": "simple",
                "is_ui": True,
                "loopback": False,
                "low_vram": False,
                # could leave the preprocessor to None and directly give the model an image of a OpenPose skeleton
                "model": "control_v11p_sd15_openpose",
                # this is most likely the preprocessor
                "module": "openpose_full",  # openpose, openpose_hand, openpose_face, openpose_faceonly, openpose_full, dw_openpose_full
                "output_dir": "",
                "pixel_perfect": True,
                "processor_res": 896,  # -1,
                "resize_mode": "Just Resize",
                "save_detected_map": True,
                "threshold_a": -1,
                "threshold_b": -1,
                "weight": 1  # from 0 to 2

            },
            # download other models: https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main
            {  # ControlNet Canny
                "advanced_weighting": None,
                "batch_images": "",
                "control_mode": "ControlNet is more important",
                "enabled": True,
                # at which part of the generation does controlnet stop affecting the output
                "guidance_end": 1,
                # at which part of the generation does controlnet start affecting the output
                "guidance_start": 0,
                "hr_option": "Both",
                "image": None,
                "inpaint_crop_input_image": True,
                "input_mode": "simple",
                "is_ui": True,
                "loopback": False,
                "low_vram": False,
                # could leave the preprocessor to None and directly give the model an image of a OpenPose skeleton
                "model": "control_v11p_sd15_canny",
                "module": "canny",  # this is most likely the preprocessor
                "output_dir": "",
                "pixel_perfect": True,
                "processor_res": 896,  # -1,
                "resize_mode": "Just Resize",
                "save_detected_map": True,
                # 0 - 255 thresholds (both low -> many edges, both high -> few edges)
                "threshold_a": 239,  # default 100
                "threshold_b": 255,  # default 200
                "weight": 2  # from 0 to 2
            },
            # {  # ControlNet Depth
            #     "advanced_weighting": None,
            #     "batch_images": "",
            #     "control_mode": "ControlNet is more important",
            #     "enabled": True,
            #     # at which part of the generation does controlnet stop affecting the output
            #     "guidance_end": 1,
            #     # at which part of the generation does controlnet start affecting the output
            #     "guidance_start": 0,
            #     "hr_option": "Both",
            #     "image": None,
            #     "inpaint_crop_input_image": True,
            #     "input_mode": "simple",
            #     "is_ui": True,
            #     "loopback": False,  # feeding back the previously generated image?
            #     "low_vram": False,
            #     # could leave the preprocessor to None and directly give the model an image of a OpenPose skeleton
            #     "model": "control_v11f1p_sd15_depth",
            #     # this is most likely the preprocessor
            #     "module": "depth_leres",  # ?depth, depth_leres, depth_leres++, depth_midas, depth_zoe, ?depth_hand_refiner, ?depth_anything
            #     "output_dir": "",
            #     "pixel_perfect": True,
            #     "processor_res": 896,  # -1,
            #     "resize_mode": "Just Resize",
            #     "save_detected_map": True,
            #     # 0 - 100 thresholds
            #     "threshold_a": 0,  # remove near % (default 0)
            #     "threshold_b": 0,  # remove background % (default 0)
            #     "weight": 1  # from 0 to 2
            # },
            # {  # ControlNet Softedge
            #     "advanced_weighting": None,
            #     "batch_images": "",
            #     "control_mode": "ControlNet is more important",
            #     "enabled": True,
            #     # at which part of the generation does controlnet stop affecting the output
            #     "guidance_end": 0.8,
            #     # at which part of the generation does controlnet start affecting the output
            #     "guidance_start": 0,
            #     "hr_option": "Both",
            #     "image": None,
            #     "inpaint_crop_input_image": True,
            #     "input_mode": "simple",
            #     "is_ui": True,
            #     "loopback": False,
            #     "low_vram": False,
            #     # could leave the preprocessor to None and directly give the model an image of a OpenPose skeleton
            #     "model": "control_v11p_sd15_canny",
            #     "module": "canny",  # this is most likely the preprocessor
            #     "output_dir": "",
            #     "pixel_perfect": True,
            #     "processor_res": 896,  # -1,
            #     "resize_mode": "Just Resize",
            #     "save_detected_map": True,
            #     "threshold_a": -1,
            #     "threshold_b": -1,
            #     "weight": 1  # from 0 to 2
            # },
            # {  # ControlNet Scribble
            #     "advanced_weighting": None,
            #     "batch_images": "",
            #     "control_mode": "ControlNet is more important",
            #     "enabled": True,
            #     # at which part of the generation does controlnet stop affecting the output
            #     "guidance_end": 0.8,
            #     # at which part of the generation does controlnet start affecting the output
            #     "guidance_start": 0,
            #     "hr_option": "Both",
            #     "image": None,
            #     "inpaint_crop_input_image": True,
            #     "input_mode": "simple",
            #     "is_ui": True,
            #     "loopback": False,
            #     "low_vram": False,
            #     # could leave the preprocessor to None and directly give the model an image of a OpenPose skeleton
            #     "model": "control_v11p_sd15_canny",
            #     "module": "canny",  # this is most likely the preprocessor
            #     "output_dir": "",
            #     "pixel_perfect": True,
            #     "processor_res": 896,  # -1,
            #     "resize_mode": "Just Resize",
            #     "save_detected_map": True,
            #     "threshold_a": -1,
            #     "threshold_b": -1,
            #     "weight": 1  # from 0 to 2
            # },
        ]},
        "Extra options": {"args": []},
        "GFPGAN": {"args": [0]},
        "Refiner": {"args": [True, "RealisticVisionV60B1.safetensors", 0.4]},  # eval?
        "Seed": {"args": [-1, False, -1, 0, 0, 0]}
    },
    "comments": {},
    "disable_extra_networks": False,
    "image_cfg_scale": 1.5,
    "refiner_checkpoint": "RealisticVisionV60B1.safetensors",
    "s_churn": 0.0,
    "s_min_uncond": 0.0,
    "s_noise": 1.0,
    "s_tmax": None,
    "s_tmin": 0.0
}